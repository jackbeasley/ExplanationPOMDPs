{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Documents/College/SymSysThesis/ExplanationPOMDPs/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"..\")"
   ]
  },
  {
   "source": [
    "## Research Question\n",
    "\n",
    "Can IBE update rules with fixed decision rules be approximated with Bayesian update rules with different decision rules? Does moving this evaluation metric (accuracy, number correct, etc) into an explicit reward distribution give us a better way to reason about the tradeoffs here?\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Precompiling ExplanationPOMDPs [56cff400-c7d8-47e0-a2d0-3cf93b18e6d3]\n└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using ExplanationPOMDPs\n",
    "using ExplanationPOMDPs.SingleObservationExplanation\n",
    "using POMDPModelTools, POMDPPolicies, POMDPSimulators, BeliefUpdaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SingleObservationPOMDP(10, 5, 0.0, 1.0, -1.0, 1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "m = SingleObservationPOMDP(10, 5, 0.0, 1.0, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FunctionPolicy{var\"#7#8\"}(var\"#7#8\"())"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "p = FunctionPolicy(s -> 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DiscreteUpdater{SingleObservationPOMDP}(SingleObservationPOMDP(10, 5, 0.0, 1.0, -1.0, 1.0))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "up = DiscreteUpdater(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "beliefs : [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "action: 5, observation: 0, state: 3\n",
      "beliefs : [0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996, 0.09999999999999996]\n",
      "action: 5, observation: 0, state: 4\n",
      "beliefs : [0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999]\n",
      "action: 5, observation: 3, state: 4\n",
      "beliefs : [0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999]\n",
      "action: 5, observation: 4, state: 7\n",
      "beliefs : [0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998]\n",
      "action: 5, observation: 3, state: 4\n",
      "beliefs : [0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998]\n",
      "action: 5, observation: 2, state: 10\n",
      "beliefs : [0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998, 0.09999999999999998]\n",
      "action: 5, observation: 5, state: 8\n",
      "beliefs : [0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999]\n",
      "action: 5, observation: 3, state: 5\n",
      "beliefs : [0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999]\n",
      "action: 5, observation: 2, state: 1\n",
      "beliefs : [0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999, 0.09999999999999999]\n",
      "action: 5, observation: 2, state: 2\n"
     ]
    }
   ],
   "source": [
    "test_simulation(m, p, up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = initialize_belief(up, isd)\n",
    "\n",
    "r_total = 0.0\n",
    "d = 1.0\n",
    "while !isterminal(pomdp, s)\n",
    "    a = action(policy, b)\n",
    "    s, o, r = gen(DDNOut(:sp,:o,:r), pomdp, s, a, rng)\n",
    "    r_total += d*r\n",
    "    d *= discount(pomdp)\n",
    "    b = update(up, b, a, o)\n",
    "end"
   ]
  }
 ]
}